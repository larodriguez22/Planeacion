{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graphics():\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Graphics module ready!\")\n",
    "        \n",
    "    def render(self,x_fin,y_fin, x, y, environment, plot_values = True):\n",
    "            \n",
    "        fig1 = plt.figure(figsize=(4, 4))\n",
    "        ax1 = fig1.add_subplot(111, aspect='equal')\n",
    "\n",
    "        # Horizontal lines.\n",
    "        for i in range(0, 6):\n",
    "            ax1.axhline(i * 0.2, linewidth=2, color=\"#2D2D33\")\n",
    "            ax1.axvline(i * 0.2, linewidth=2, color=\"#2D2D33\")\n",
    "\n",
    "        # Salida, Meta & GameOver.\n",
    "        # Salida (amarillos)\n",
    "        ax1.add_patch(patches.Rectangle((0.0, 0.0), 0.2, 0.2, facecolor = \"#F6D924\"))\n",
    "        \n",
    "        ax1.add_patch(patches.Rectangle((0.2, 0.8), 0.2, 0.2, facecolor = \"#F6D924\"))\n",
    "        ax1.add_patch(patches.Rectangle((0.8, 0.2), 0.2, 0.2, facecolor = \"#F6D924\"))\n",
    "        \n",
    "        # Meta (verde)\n",
    "        ax1.add_patch(patches.Rectangle((x_fin,y_fin), 0.2, 0.2, facecolor = \"#68FF33\"))\n",
    "        #ax1.add_patch(patches.Rectangle((0.8, 0.8), 0.2, 0.2, facecolor = \"#FF5533\"))\n",
    "        \n",
    "        # Muros del juego.\n",
    "        array=[[0.8, 0.6],[0.2, 0.4],[0.6, 0.2],[0.2, 0.0],[0.4, 0.8],[0.8, 0.4]]\n",
    "        while len(array)!=0:\n",
    "            var2 = array.pop(0)\n",
    "            x_ = var2[0]\n",
    "            y_ = var2[1]\n",
    "            ax1.add_patch(patches.Rectangle((x_,y_), 0.2, 0.2, facecolor = \"#33A4FF\"))\n",
    "        #ax1.add_patch(patches.Rectangle((0.6, 0.2), 0.2, 0.2, facecolor = \"#33A4FF\"))\n",
    "        #ax1.add_patch(patches.Rectangle((0.2, 0.0), 0.2, 0.2, facecolor = \"#33A4FF\"))\n",
    "        \n",
    "        #ax1.add_patch(patches.Rectangle((0.4, 0.8), 0.2, 0.2, facecolor = \"#33A4FF\"))\n",
    "        #ax1.add_patch(patches.Rectangle((0.4, 0.8), 0.2, 0.2, facecolor = \"#33A4FF\"))\n",
    "        #ax1.add_patch(patches.Rectangle((0.8, 0.4), 0.2, 0.2, facecolor = \"#33A4FF\"))\n",
    "        \n",
    "        # Limit grid view.\n",
    "        plt.ylim((0, 1))\n",
    "        plt.xlim((0, 1))\n",
    "\n",
    "        # Plot player.\n",
    "        plt.scatter(x, y, s = 100, color = \"black\", marker = \"o\", facecolor = \"blue\", edgecolors = \"blue\", zorder = 10)\n",
    "\n",
    "        # Plot state values.\n",
    "        if plot_values:\n",
    "            for i in range(0, len(environment.value_state_table)):\n",
    "                for j in range(0, len(environment.value_state_table[0])):\n",
    "                    plt.text(environment.grid_pos[i] - 0.08, environment.grid_pos[j] - 0.03, \n",
    "                             round(environment.value_state_table[i][j], 1), fontsize=16)\n",
    "                \n",
    "        # Plot grid.\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridEnvironment():\n",
    "    \n",
    "    def _init_(self, fin_x, fin_y, route_img):\n",
    "        self.fin_x = fin_x\n",
    "        self.fin_y = fin_y\n",
    "        \n",
    "        # RECOMPENSA\n",
    "        self.rw = -1 # Living (Movement) Penalty\n",
    "        \n",
    "        # CARGA IMAGEN\n",
    "        im = Image.open(route_img, 'r')\n",
    "        \n",
    "        # DIMENSIONES\n",
    "        self.columns = im.size[0]\n",
    "        self.rows = im.size[1]\n",
    "        \n",
    "        # INICIALIZACION\n",
    "        pix_val = list(im.getdata())\n",
    "        self.new_pix = [x[0] for x in pix_val]\n",
    "        self.walls_and_paths = np.ones([self.rows,self.columns])\n",
    "        for i in range(self.rows):\n",
    "            for j in range(self.columns):\n",
    "                if self.new_pix[i*self.columns+j] == 0:\n",
    "                    self.walls_and_paths[i,j] = 0\n",
    "        self.rewards = self.rw*np.ones([self.rows,self.columns])\n",
    "        \n",
    "        # Cambiar el valor\n",
    "        self.value_state_table = np.zeros([self.rows,self.columns])\n",
    "        self.value_state_table[fin_x][fin_y]=100\n",
    "    \n",
    "    def getStateValue(self, position):\n",
    "        return self.value_state_table[position[0]][position[1]]\n",
    "    \n",
    "    def reset(self, fin_x, fin_y):\n",
    "        self.value_state_table = np.zeros([self.rows,self.columns])\n",
    "        self.value_state_table[fin_x][fin_y]=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x  2\n",
      "y  3\n",
      "x final:  2\n",
      "y final:  3\n"
     ]
    }
   ],
   "source": [
    "x_fin=0\n",
    "y_fin=0\n",
    "[0.8, 0.4]\n",
    "while (x_fin==0 and y_fin==0) or (x_fin==1 and y_fin==4) or (x_fin==4 and y_fin==1) or (x_fin==4 and y_fin==3) or (x_fin==1 and y_fin==2) or(x_fin==3 and y_fin==1) or(x_fin==1 and y_fin==0) or(x_fin==2 and y_fin==4) or(x_fin==4 and y_fin==2):\n",
    "    x_fin=random.randint(1, 5)-1\n",
    "    y_fin=random.randint(1, 5)-1\n",
    "    print(\"x \", x_fin)\n",
    "    print(\"y \",y_fin)\n",
    "\n",
    "print(\"x final: \",x_fin)\n",
    "print(\"y final: \",y_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphics module ready!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "environment = GridEnvironment(x_fin,y_fin)\n",
    "graph = Graphics()\n",
    "graph.render(x_fin*0.2,y_fin*0.2, 0.1, 0.1, environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the position array is created\n",
    "array_pos=[]\n",
    "\n",
    "class valueBasedAgent():\n",
    "    \n",
    "    def __init__(self, environment, policy, discount_factor):\n",
    "        self.pos = [0,0]\n",
    "        self.total_reward = 0\n",
    "        self.environment = environment\n",
    "        self.discount_factor = discount_factor\n",
    "        self.actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "        \n",
    "        # Start with a random policy. 0.25 chance of moving to any direction.\n",
    "        self.policy = policy   \n",
    "            \n",
    "    def forwardState(self, pos, action):\n",
    "        \n",
    "        # New position array.\n",
    "        new_position = pos\n",
    "        \n",
    "        # Compute new position based on action taken.\n",
    "        if(action == \"up\" and pos[1] < 4):\n",
    "            if(self.environment.walls_and_paths[pos[0]][pos[1] + 1]) == 1:\n",
    "                new_position = [pos[0], pos[1] + 1]\n",
    "\n",
    "        elif(action == \"down\" and pos[1] > 0):\n",
    "            if(self.environment.walls_and_paths[pos[0]][pos[1] - 1]) == 1:\n",
    "                new_position = [pos[0], pos[1] - 1]\n",
    "                \n",
    "        elif(action == \"left\" and pos[0] > 0):\n",
    "            if(self.environment.walls_and_paths[pos[0] - 1][pos[1]]) == 1:\n",
    "                new_position = [pos[0] - 1, pos[1]]\n",
    "\n",
    "        elif(action == \"right\" and pos[0] < 4):\n",
    "            if(self.environment.walls_and_paths[pos[0] + 1][pos[1]]) == 1:\n",
    "                new_position = [pos[0] + 1, pos[1]]\n",
    "        return new_position\n",
    "        \n",
    "        \n",
    "    def valueFunction(self):\n",
    "            \n",
    "        # Initialize variable.\n",
    "        new_state_value = 0\n",
    "    \n",
    "        # Random movement! - Cuando aun no se ha inicializado \n",
    "        if self.policy[self.pos[0]][self.pos[1]] == \"r\":\n",
    "            for action in self.actions:        \n",
    "                forward_state = self.forwardState(self.pos, action)\n",
    "                \n",
    "                # Simplified version of Q-value. BELLMANS EQUATION\n",
    "                q_value = (self.environment.rewards[forward_state[0]][forward_state[1]] \n",
    "                                    + self.discount_factor * self.environment.value_state_table[forward_state[0]][forward_state[1]])\n",
    "                new_state_value += q_value * 0.25 # Probabilidad de 0.25 para cada una de las acciones\n",
    "            return new_state_value\n",
    "        \n",
    "        # Not random movement!\n",
    "        else: \n",
    "            action = self.policy[self.pos[0]][self.pos[1]]\n",
    "            forward_state = self.forwardState(self.pos, action)\n",
    "            \n",
    "            # Simplified version of Q-value.\n",
    "            q_value = (self.environment.rewards[forward_state[0]][forward_state[1]] \n",
    "                                    + self.discount_factor * self.environment.value_state_table[forward_state[0]][forward_state[1]])\n",
    "            new_state_value += q_value # Probabilidad de 1\n",
    "            return new_state_value\n",
    "        \n",
    "    def getPosition(self):\n",
    "        return self.pos\n",
    "    \n",
    "    def getReward(self):\n",
    "        return self.total_reward\n",
    "    \n",
    "    def setPosition(self, x, y):\n",
    "        self.pos = [x, y]\n",
    "        \n",
    "    def updateValueStateTable(self):\n",
    "        new_state_value = self.valueFunction()\n",
    "        self.environment.value_state_table[self.pos[0]][self.pos[1]] = new_state_value\n",
    "        \n",
    "    def selectBestAction(self):\n",
    "        \n",
    "        # Compute new possible states.\n",
    "        go_up = self.forwardState(self.pos, \"up\")\n",
    "        go_down = self.forwardState(self.pos, \"down\")\n",
    "        go_left = self.forwardState(self.pos, \"left\")\n",
    "        go_right = self.forwardState(self.pos, \"right\")\n",
    "        \n",
    "        # Q values (simplified version).\n",
    "        up_value = (self.environment.rewards[go_up[0]][go_up[1]] + \n",
    "                    self.discount_factor * self.environment.value_state_table[go_up[0]][go_up[1]])\n",
    "        down_value = (self.environment.rewards[go_down[0]][go_down[1]] + \n",
    "                      self.discount_factor * self.environment.value_state_table[go_down[0]][go_down[1]])\n",
    "        left_value = (self.environment.rewards[go_left[0]][go_left[1]] + \n",
    "                        self.discount_factor * self.environment.value_state_table[go_left[0]][go_left[1]])\n",
    "        right_value = (self.environment.rewards[go_right[0]][go_right[1]] + \n",
    "                       self.discount_factor * self.environment.value_state_table[go_right[0]][go_right[1]])\n",
    "        \n",
    "        # Array of Q-values.\n",
    "        values = [up_value, down_value, left_value, right_value]\n",
    "        \n",
    "        best_action = self.actions[values.index(max(values))] \n",
    "        return best_action       \n",
    "            \n",
    "    def move(self):\n",
    "    \n",
    "        # Select action according to policy.\n",
    "        action = self.policy[self.pos[0]][self.pos[1]]\n",
    "        print(\"Action taken\", action)\n",
    "\n",
    "        # Move to new position according to action taken.\n",
    "        self.pos = self.forwardState(self.pos, action)\n",
    "        \n",
    "        print(\"New Position: \", self.pos)\n",
    "        \n",
    "        # the current position is added to the position array\n",
    "        array_pos.append (self.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyEvaluation(Graphics):\n",
    "    \n",
    "    def __init__(self,x_fin,y_fin, environment, agent, iterations = 3):\n",
    "        \n",
    "        self.x_fin = x_fin\n",
    "        self.y_fin = y_fin\n",
    "        self.environment = environment       \n",
    "        self.agent = agent                     \n",
    "        #print(\"GridWorld Initialize!\")\n",
    "        self.iterations = iterations\n",
    "    \n",
    "    def evaluate(self, plot_grid = True):\n",
    "        self.DP_policy_evaluation(self.iterations, plot_grid)\n",
    "        \n",
    "    def DP_policy_evaluation(self, iterations, plot_grid):\n",
    "        \n",
    "        for k in range(0, iterations):\n",
    "            for i in range(0, len(self.environment.value_state_table)):\n",
    "                for j in range(0, len(self.environment.value_state_table[0])):\n",
    "\n",
    "                    if self.environment.walls_and_paths[i][j] == 1 and self.canChangeStateValue(i, j):\n",
    "\n",
    "                        # Set agent position.\n",
    "                        self.agent.setPosition(i, j)\n",
    "                        self.agent.updateValueStateTable()\n",
    "\n",
    "                        # Method of the super class.\n",
    "                        if(plot_grid):\n",
    "                            \n",
    "                            # Render game.\n",
    "                            pos = self.agent.getPosition()\n",
    "                            grid_coords = self.environment.grid_pos\n",
    "                            \n",
    "                            self.render(grid_coords[pos[0]], grid_coords[pos[1]], self.environment, True)\n",
    "                            time.sleep(0.01)\n",
    "                            clear_output(wait = True)\n",
    "                            \n",
    "    \n",
    "\n",
    "    def canChangeStateValue(self, x, y):\n",
    "        # Posicion que no se puede modificar\n",
    "        cant_modify = bool((x == self.x_fin and y == self.y_fin)) # or (x == 4 and y == 4))\n",
    "        \n",
    "        grid = self.environment.walls_and_paths\n",
    "        coords = list()\n",
    "        \n",
    "        # Get walls.\n",
    "        for i in range(0, len(grid)):\n",
    "            for j in range(0, len(grid[0])):\n",
    "                if grid[i][j] == 0:\n",
    "                    coords.append([i, j])\n",
    "        for c in coords: \n",
    "            if c == [x, y]:\n",
    "                cant_modify = True\n",
    "                break\n",
    "                \n",
    "        return not cant_modify\n",
    "    \n",
    "    def updatePolicy(self):\n",
    "        \n",
    "         for i in range(0, len(self.environment.value_state_table)):\n",
    "                for j in range(0, len(self.environment.value_state_table[0])):\n",
    "                    if self.environment.walls_and_paths[i][j] == 1:\n",
    "                        \n",
    "                        # Set agent position.\n",
    "                        self.agent.setPosition(i, j)\n",
    "                        best_action = self.agent.selectBestAction()\n",
    "                        self.agent.policy[i][j] = best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Game(Graphics):\n",
    "    \n",
    "    def __init__(self,x_fin,y_fin, environment, agent):\n",
    "        \n",
    "        self.environment = environment       \n",
    "        self.agent = agent             \n",
    "        print(\"GridWorld Initialize!\")\n",
    "                \n",
    "    def update(self, secs):\n",
    "        route = []\n",
    "        pos = self.agent.getPosition()\n",
    "        grid_coords = self.environment.grid_pos\n",
    "        self.render(x_fin*0.2,y_fin*0.2, grid_coords[pos[0]], grid_coords[pos[1]], self.environment, False)\n",
    "        time.sleep(1)\n",
    "        clear_output(wait = True)\n",
    "        # (self.agent.pos[0] == 4 and self.agent.pos[1] == 3)\n",
    "        while not ((self.agent.pos[0] == x_fin and self.agent.pos[1] == y_fin)):\n",
    "            \n",
    "            self.agent.move()\n",
    "            pos = self.agent.getPosition()\n",
    "            print(grid_coords[pos[0]], grid_coords[pos[1]])\n",
    "            self.render(x_fin*0.2,y_fin*0.2, grid_coords[pos[0]], grid_coords[pos[1]], self.environment, False)\n",
    "            time.sleep(secs)\n",
    "            clear_output(wait = True)\n",
    "            \n",
    "        \n",
    "        \n",
    "        #self.render(grid_coords[pos[0]], grid_coords[pos[1]], self.environment, False)\n",
    "        #time.sleep(secs)\n",
    "        #print(\"Yuhuu, we won the game!\")\n",
    "        #clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Policy Evaluation (Planning) for DP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['r', 'r', 'r', 'r', 'r'],\n",
       " ['r', 'r', 'r', 'r', 'r'],\n",
       " ['r', 'r', 'r', 'r', 'r'],\n",
       " ['r', 'r', 'r', 'r', 'r'],\n",
       " ['r', 'r', 'r', 'r', 'r']]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the random policy.\n",
    "policy = list()\n",
    "for i in range(0, 5):\n",
    "    column = list()\n",
    "    for j in range(0, 5):\n",
    "        column.append(\"r\")\n",
    "    policy.append(column)\n",
    "\n",
    "# Initialize environment and agent.\n",
    "discount_factor = 1\n",
    "environment = GridEnvironment(x_fin,y_fin)\n",
    "agent = valueBasedAgent(environment, policy, discount_factor)\n",
    "\n",
    "agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 100, 0],\n",
       " [0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.value_state_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize policy evaluation class.\n",
    "policy_evaluation = PolicyEvaluation(x_fin,y_fin,environment, agent, iterations = 100)\n",
    "policy_evaluation.evaluate(plot_grid = False)\n",
    "\n",
    "policy_evaluation.updatePolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['up', 'right', 'down', 'down', 'down'],\n",
       " ['r', 'right', 'r', 'r', 'left'],\n",
       " ['up', 'up', 'up', 'up', 'r'],\n",
       " ['left', 'r', 'up', 'left', 'down'],\n",
       " ['left', 'down', 'r', 'left', 'down']]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New agent policy after policy evaluation.\n",
    "agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-30.434679247143414,\n",
       "  -27.123817080657684,\n",
       "  -40.36664287544215,\n",
       "  -50.226106826525466,\n",
       "  -56.7661791211107],\n",
       " [0, -7.060797987722489, 0, 0, -60.027904695367994],\n",
       " [1.7541152182946886, 16.669297615237078, 59.19834367903702, 100, 0],\n",
       " [-9.492646496427973,\n",
       "  0,\n",
       "  64.86836912596219,\n",
       "  74.48272771300327,\n",
       "  68.52149010526948],\n",
       " [-16.990442699364984,\n",
       "  -20.73936810769016,\n",
       "  0,\n",
       "  68.52149010526948,\n",
       "  66.53109054364663]]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.value_state_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Win the Game with the previous policy evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action taken up\n",
      "New Position:  [2, 3]\n",
      "0.5 0.7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.pos = [4, 1]\n",
    "array_pos.append (agent.pos)\n",
    "game = Game(x_fin,y_fin, environment, agent)\n",
    "game.update(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 1], [4, 0], [3, 0], [2, 0], [2, 1], [2, 2], [2, 3]]\n"
     ]
    }
   ],
   "source": [
    "print (array_pos)\n",
    "#agent.pos = [4, 1]\n",
    "#game = Game(x_fin,y_fin,environment, agent)\n",
    "#game.update(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#agent.pos = [0, 0]\n",
    "#game = Game(x_fin,y_fin,environment, agent)\n",
    "#game.update(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the random policy.\n",
    "policy = list()\n",
    "for i in range(0, 5):\n",
    "    column = list()\n",
    "    for j in range(0, 5):\n",
    "        column.append(\"r\")\n",
    "    policy.append(column)\n",
    "    \n",
    "# Initaliza environment and agent.\n",
    "discount_factor = 0.5\n",
    "environment = GridEnvironment(x_fin,y_fin)\n",
    "agent = valueBasedAgent(environment, policy, discount_factor)\n",
    "\n",
    "# Policy iteration algorithm.\n",
    "for i in range(0, 1000):\n",
    "\n",
    "    # Reset value function.\n",
    "    environment.reset(x_fin,y_fin)\n",
    "\n",
    "    # Evaluate new policy.\n",
    "    policy_evaluation = PolicyEvaluation(x_fin,y_fin, environment, agent, iterations = 10)\n",
    "    policy_evaluation.evaluate(plot_grid = False)\n",
    "    policy_evaluation.updatePolicy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['up', 'right', 'down', 'down', 'down'],\n",
       " ['r', 'right', 'r', 'r', 'left'],\n",
       " ['up', 'up', 'up', 'up', 'r'],\n",
       " ['left', 'r', 'up', 'left', 'down'],\n",
       " ['left', 'down', 'r', 'left', 'down']]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the random policy.\n",
    "policy = list()\n",
    "for i in range(0, 5):\n",
    "    column = list()\n",
    "    for j in range(0, 5):\n",
    "        column.append(\"r\")\n",
    "    policy.append(column)\n",
    "    \n",
    "# Initaliza environment and agent.\n",
    "discount_factor = 0.6\n",
    "environment = GridEnvironment(x_fin,y_fin)\n",
    "agent = valueBasedAgent(environment, policy, discount_factor)\n",
    "\n",
    "# Policy iteration algorithm.\n",
    "for i in range(0, 1000):\n",
    "\n",
    "    # Reset value function.\n",
    "    # environment.reset() => We do not reset the environment? \n",
    "\n",
    "    # Evaluate new policy.\n",
    "    policy_evaluation = PolicyEvaluation(x_fin,y_fin,environment, agent, iterations = 1)\n",
    "    policy_evaluation.evaluate(plot_grid = False)\n",
    "    policy_evaluation.updatePolicy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['up', 'right', 'down', 'down', 'down'],\n",
       " ['r', 'right', 'r', 'r', 'left'],\n",
       " ['up', 'up', 'up', 'up', 'r'],\n",
       " ['left', 'r', 'up', 'left', 'down'],\n",
       " ['left', 'down', 'r', 'left', 'down']]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 0], [2, 0], [2, 3]]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function that debugs the position array and returns only the desired coordinates\n",
    "def coordenates (route:list):\n",
    "    final_coordenates=[]\n",
    "    \n",
    "    for i in range (1,len (route) -1):\n",
    "        if route [i][0] == route [i-1][0] :\n",
    "            if route [i][0] != route [i+1][0] :\n",
    "                final_coordenates.append (route[i])\n",
    "        elif route [i][1] == route [i-1][1] :\n",
    "            if route [i][1] != route [i+1][1] :\n",
    "                final_coordenates.append (route[i])                      \n",
    "    final_coordenates.append (route[len (route)-1])        \n",
    "    return final_coordenates\n",
    "coordenates (array_pos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the array is reinitialized\n",
    "array_pos=[]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
